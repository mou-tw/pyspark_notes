{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": "NULL value exception in pyspark and how to handle",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 認知 NULL值在spark是特殊的"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkcluster01",
              "session_id": "10963",
              "statement_id": 26,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-13T02:59:30.4511663Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-13T02:59:30.545766Z",
              "execution_finish_time": "2022-09-13T02:59:31.0673046Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(sparkcluster01, 10963, 26, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+----+\n| id|foo| bar|\n+---+---+----+\n|  a|  1|   1|\n|  b|  2|   1|\n|  c|  1|null|\n+---+---+----+\n\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "df=spark.createDataFrame([('a',1,1),('b',2,1),('c',1,None)],\r\n",
        "                            schema=('id', 'foo', 'bar')\r\n",
        "                        )\r\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Target: 想要找出foo=1 且 bar不等於1的id，預期要是id=c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkcluster01",
              "session_id": "10963",
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-13T02:44:31.3476033Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-13T02:44:31.4353884Z",
              "execution_finish_time": "2022-09-13T02:44:32.5113056Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(sparkcluster01, 10963, 17, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+\n| id|foo|bar|\n+---+---+---+\n+---+---+---+\n\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#測試\r\n",
        "df.filter( (df.foo==1) & (df.bar!=1) ).show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "預期: id=c bar值為null不等於1，且foo=1，應該要取得id=c的資料 \r\n",
        "\r\n",
        "實際: 沒有配對到任何資料\r\n",
        "\r\n",
        "**結果: 失敗**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### example 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkcluster01",
              "session_id": "10963",
              "statement_id": 28,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-13T03:07:06.4327193Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-13T03:07:06.5387259Z",
              "execution_finish_time": "2022-09-13T03:07:08.4156368Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(sparkcluster01, 10963, 28, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+-----+\n|  id1| id2|value|\n+-----+----+-----+\n|test1|  19|    5|\n|test2|   0|   19|\n|test3|null|   95|\n+-----+----+-----+\n\n+------+----+-----+\n|   id1| id2|value|\n+------+----+-----+\n| test3|null|    5|\n|test22|   5| 9487|\n+------+----+-----+\n\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df1=spark.createDataFrame([('test1',19,5),('test2',0,19),('test3',None,95)],\r\n",
        "                            schema=('id1','id2','value')\r\n",
        "                        )\r\n",
        "\r\n",
        "df2=spark.createDataFrame([('test3',None,5),('test22',5,9487)],\r\n",
        "                            schema=('id1','id2','value')\r\n",
        "                        )\r\n",
        "\r\n",
        "df1.show()\r\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "希望用delete insert方式結合B資料，使用join的leftanti方式，在union B\r\n",
        "\r\n",
        "![](https://miro.medium.com/max/963/1*czqgln7uArxtXOqnw1THbA.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkcluster01",
              "session_id": "10963",
              "statement_id": 31,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-13T03:07:50.7741912Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-13T03:07:50.8692774Z",
              "execution_finish_time": "2022-09-13T03:07:57.7164618Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(sparkcluster01, 10963, 31, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+-----+\n|   id1| id2|value|\n+------+----+-----+\n| test3|null|   95|\n| test2|   0|   19|\n| test1|  19|    5|\n| test3|null|    5|\n|test22|   5| 9487|\n+------+----+-----+\n\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df1.join(df2,['id1','id2'],how='leftanti').union(df2).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "預期: df1與df2共同擁有id1=test3, id2=Null，為重複key值，在A.join(left-anti)時會捨棄df1的id=test3，union df2補回來id1=test3，\r\n",
        "更新id1=test3, id2=None的 value。\r\n",
        "\r\n",
        "實際: df1與df2的id1=test3,id2=None 兩筆資料都保留下來\r\n",
        "\r\n",
        "**結果: 失敗**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 小結: NULL 沒辦法判斷大小、等於非等於，只能使用isNull isNotNull來判斷是否是NULL值，其餘為未定義\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 官方解方 <=> or eqNullSafe\r\n",
        "\r\n",
        "參考資料:\r\n",
        "\r\n",
        "[NULL semantic](https://spark.apache.org/docs/3.0.0-preview/sql-ref-null-semantics.html)\r\n",
        "\r\n",
        "[eqNullSafe](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.Column.eqNullSafe.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Example 1 : 正確語法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkcluster01",
              "session_id": "10963",
              "statement_id": 32,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-13T03:26:21.2609141Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-13T03:26:21.3720011Z",
              "execution_finish_time": "2022-09-13T03:26:22.44656Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(sparkcluster01, 10963, 32, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+----+\n| id|foo| bar|\n+---+---+----+\n|  c|  1|null|\n+---+---+----+\n\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.filter( (df.foo==1) & ((df.bar.isNull()) | (df.bar!=1)) ).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkcluster01",
              "session_id": "10963",
              "statement_id": 33,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-13T03:26:31.5467024Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-13T03:26:31.6290437Z",
              "execution_finish_time": "2022-09-13T03:26:32.1699649Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(sparkcluster01, 10963, 33, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+----+\n| id|foo| bar|\n+---+---+----+\n|  c|  1|null|\n+---+---+----+\n\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.filter( (df.foo==1) & ~(df.bar.eqNullSafe(1)) ).show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Example 2: 正確語法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkcluster01",
              "session_id": "10963",
              "statement_id": 34,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-13T03:28:24.0012116Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-13T03:28:24.1668395Z",
              "execution_finish_time": "2022-09-13T03:28:24.3632468Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(sparkcluster01, 10963, 34, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Column<'(id1 <=> id1)'>, Column<'(id2 <=> id2)'>]"
          },
          "execution_count": 103,
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "_key_col= ['id1','id2']\r\n",
        "\r\n",
        "join_cond= [df1[k].eqNullSafe(df2[k]) for k in _key_col] #讓NULL可以判斷等於NULL 透過 eqNullSafe\r\n",
        "\r\n",
        "join_cond"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkcluster01",
              "session_id": "10963",
              "statement_id": 35,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-09-13T03:28:55.053731Z",
              "session_start_time": null,
              "execution_start_time": "2022-09-13T03:28:55.1999968Z",
              "execution_finish_time": "2022-09-13T03:28:59.2945772Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(sparkcluster01, 10963, 35, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+-----+\n|   id1| id2|value|\n+------+----+-----+\n| test2|   0|   19|\n| test1|  19|    5|\n| test3|null|    5|\n|test22|   5| 9487|\n+------+----+-----+\n\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df1.join(df2,join_cond,how='leftanti').union(df2).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 小結: 透過上面可以找出如何使用官方提供方式，讓NULL = NULL 是True，而非未定義，進而得到預期結果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 結論:\r\n",
        "當判斷的col有NULL值時，任何邏輯判斷需要特別特別注意，很容易出現異想不倒的情況，需要提前排除，透過官方提供方式，能夠有效避免問題"
      ]
    }
  ]
}